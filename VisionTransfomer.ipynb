{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTImageProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os \n",
    "#import wandb\n",
    "from tqdm import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shoba\\ML\\env\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(\"final_dataset/train\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"final_dataset/train\"\n",
    "test_dir = \"final_dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, feature_extractor, labels):\n",
    "        self.feature_extractor = feature_extractor \n",
    "        self.files = []\n",
    "        self.labels = labels\n",
    "        \n",
    "        # get all the subdirectories in the root folder\n",
    "        subdirs = sorted(os.listdir(root))\n",
    "        \n",
    "        for subdir in subdirs:\n",
    "            for path, _, files in os.walk(os.path.join(root, subdir)):\n",
    "                for file in files:\n",
    "                    # append both the filename and label\n",
    "                    self.files.append((os.path.join(root, subdir, file), subdir))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.files[index % len(self.files)]\n",
    "        img = Image.open(img).convert(\"RGB\")\n",
    "        img = self.feature_extractor(img, return_tensors=\"pt\")\n",
    "        img[\"labels\"] = torch.tensor(self.labels.index(label))\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_dir, feature_extractor, classes)\n",
    "test_dataset = ImageDataset(test_dir, feature_extractor, classes)\n",
    "\n",
    "\n",
    "test_train_size = int(0.8 * len(test_dataset))\n",
    "test_test_size = len(test_dataset) - test_train_size\n",
    "\n",
    "\n",
    "test_dataset, val_dataset = torch.utils.data.random_split(test_dataset, [test_train_size, test_test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1f101ea3d00>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1f101ea3550>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1f101ea3430>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8838ce0b880742238cda613c7107c99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfba583af8146349700ac8e908e4e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/961 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (C:/Users/shoba/.cache/huggingface/datasets/imagefolder/default-6a4e43a2077b7a98/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91987071829847bf9b0b7c379564b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1838\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 960\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch = next(enumerate(train_loader))\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imagefolder\", data_dir=\"final_dataset\")\n",
    "ds\n",
    "\n",
    "\n",
    "#for batch in train_loader:\n",
    "   # print(batch)\n",
    "   # break\n",
    "\n",
    "#batch = train_loader[0]\n",
    "#batch_images = batch[\"pixel_values\"]\n",
    "#batch_labels = batch[\"labels\"]\n",
    "\n",
    "#plt.figure(figsize=(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['Adult', 'Airplane', 'Alpaca', 'Bird', 'Bus', 'Car', 'Cat', 'Child', 'Elephant', 'Flower', 'Giraffe', 'Horse', 'Monkey', 'Panda', 'Reptile', 'Vessel'], id=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ds[\"train\"].features[\"label\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "  # convert all images to RGB format, then preprocessing it\n",
    "  # using our image processor\n",
    "  inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
    "  # we also shouldn't forget about the labels\n",
    "  inputs[\"labels\"] = examples[\"label\"]\n",
    "  return inputs\n",
    "\n",
    "# use the with_transform() method to apply the transform to the dataset on the fly during training\n",
    "dataset = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adult',\n",
       " 'Airplane',\n",
       " 'Alpaca',\n",
       " 'Bird',\n",
       " 'Bus',\n",
       " 'Car',\n",
       " 'Cat',\n",
       " 'Child',\n",
       " 'Elephant',\n",
       " 'Flower',\n",
       " 'Giraffe',\n",
       " 'Horse',\n",
       " 'Monkey',\n",
       " 'Panda',\n",
       " 'Reptile',\n",
       " 'Vessel']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for item in dataset[\"train\"]:\n",
    "  print(item[\"pixel_values\"].shape)\n",
    "  print(item[\"labels\"])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  return {\n",
    "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "# load the accuracy and f1 metrics from the evaluate module\n",
    "accuracy = load(\"accuracy\")\n",
    "f1 = load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  # compute the accuracy and f1 scores & return them\n",
    "  accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
    "  f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
    "  return {**accuracy_score, **f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([16, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([16]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the ViT model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([16, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([16]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_epochs = 5\n",
    "lr = 2e-4\n",
    "eval_steps = 100\n",
    "record_steps= 10\n",
    "save_checkpoint = 5\n",
    "\n",
    "#wandb.config.update({\"lr\": lr, \"num_epochs\": num_epochs})\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=len(classes),\n",
    "                                                   id2label={str(i): c for i, c in enumerate(classes)},\n",
    "                                                   label2id={c: str(i) for i, c in enumerate(classes)},\n",
    "                                                   ignore_mismatched_sizes=True).to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# negative log likelihood -> multi-class classification\n",
    "training_stats_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-custom, # output directory\n",
    "  # output_dir=\"./vit-base-skin-cancer\",\n",
    "  per_device_train_batch_size=32, # batch size per device during training\n",
    "  evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
    "  num_train_epochs=50,             # total number of training epochs\n",
    "  # fp16=True,                    # use mixed precision\n",
    "  save_steps=1000,                # number of update steps before saving checkpoint\n",
    "  eval_steps=1000,                # number of update steps before evaluating\n",
    "  logging_steps=1000,             # number of update steps before logging\n",
    "  # save_steps=50,\n",
    "  # eval_steps=50,\n",
    "  # logging_steps=50,\n",
    "  save_total_limit=2,             # limit the total amount of checkpoints on disk\n",
    "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
    "  push_to_hub=False,              # do not push the model to the hub\n",
    "  report_to='tensorboard',        # report metrics to tensorboard\n",
    "  load_best_model_at_end=True,    # load the best model at the end of training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                 # training arguments, defined above\n",
    "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
    "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
    "    train_dataset=dataset[\"train\"],     # training dataset\n",
    "    eval_dataset=dataset[\"test\"], # evaluation dataset\n",
    "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shoba\\ML\\env\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006a2292a47747cba2637169bcbc070a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192639be6211401f88df72c8a0bc69c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09072335064411163,\n",
       " 'eval_accuracy': 0.9833333333333333,\n",
       " 'eval_f1': 0.983288697020956,\n",
       " 'eval_runtime': 395.7325,\n",
       " 'eval_samples_per_second': 2.426,\n",
       " 'eval_steps_per_second': 0.303,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shoba\\ML\\env\\lib\\site-packages\\torchinfo\\torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "c:\\Users\\shoba\\ML\\env\\lib\\site-packages\\torch\\storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n",
       "======================================================================================================================================================\n",
       "ViTForImageClassification (ViTForImageClassification)                  [32, 3, 224, 224]    [32, 16]             --                   True\n",
       "â”œâ”€ViTModel (vit)                                                       [32, 3, 224, 224]    [32, 197, 768]       --                   True\n",
       "â”‚    â””â”€ViTEmbeddings (embeddings)                                      [32, 3, 224, 224]    [32, 197, 768]       152,064              True\n",
       "â”‚    â”‚    â””â”€ViTPatchEmbeddings (patch_embeddings)                      [32, 3, 224, 224]    [32, 196, 768]       590,592              True\n",
       "â”‚    â”‚    â””â”€Dropout (dropout)                                          [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "â”‚    â””â”€ViTEncoder (encoder)                                            [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "â”‚    â”‚    â””â”€ModuleList (layer)                                         --                   --                   85,054,464           True\n",
       "â”‚    â””â”€LayerNorm (layernorm)                                           [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "â”œâ”€Linear (classifier)                                                  [32, 768]            [32, 16]             12,304               True\n",
       "======================================================================================================================================================\n",
       "Total params: 85,810,960\n",
       "Trainable params: 85,810,960\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.43\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 5189.87\n",
       "Params size (MB): 342.64\n",
       "Estimated Total Size (MB): 5551.77\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from torch.optim.lr_scheduler import StepLR\\n\\nscheduler = StepLR(optim, step_size=1, gamma=0.6)'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "scheduler = StepLR(optim, step_size=1, gamma=0.6)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def evaluate(model, eval_loader, eval_dataset):\\n    correct = 0\\n    eval_loss = []\\n    \\n    model.eval()\\n    for batch in tqdm(eval_loader):\\n        # extracting images and labels from batch \\n        batch_images = batch[\"pixel_values\"].squeeze(1).to(device)\\n        batch_labels = batch[\"labels\"].to(device)\\n        \\n        # not training the model\\n        with torch.no_grad():\\n            outputs = model(pixel_values=batch_images, labels=batch_labels)\\n            loss = outputs[0]\\n            eval_loss.append(loss.item())\\n            correct += (torch.argmax(outputs[\"logits\"], dim=1) == batch_labels).sum().item()\\n    \\n    # return eval accuracy and loss\\n    accuracy = (100 * correct / len(eval_dataset))\\n    #avg_loss = (sum(eval_loss) / len(epoch_loss))\\n    \\n    return accuracy#, avg_loss'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def evaluate(model, eval_loader, eval_dataset):\n",
    "    correct = 0\n",
    "    eval_loss = []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_loader):\n",
    "        # extracting images and labels from batch \n",
    "        batch_images = batch[\"pixel_values\"].squeeze(1).to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # not training the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=batch_images, labels=batch_labels)\n",
    "            loss = outputs[0]\n",
    "            eval_loss.append(loss.item())\n",
    "            correct += (torch.argmax(outputs[\"logits\"], dim=1) == batch_labels).sum().item()\n",
    "    \n",
    "    # return eval accuracy and loss\n",
    "    accuracy = (100 * correct / len(eval_dataset))\n",
    "    #avg_loss = (sum(eval_loss) / len(epoch_loss))\n",
    "    \n",
    "    return accuracy#, avg_loss'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tqdm = partial(tqdm, position=0, leave=True)\\n\\n\\nfor epoch in range(1, num_epochs+1):\\n    # storing loss and accuracy across the epoch\\n    epoch_loss = []\\n    epoch_acc = []\\n    \\n    print(f\"Epoch {epoch}\")\\n    for index, batch in enumerate(tqdm(train_loader)):\\n        model.train()\\n        optim.zero_grad()\\n\\n        # extract images and labels from batch\\n        batch_images = batch[\"pixel_values\"].squeeze(1).to(device)\\n        batch_labels = batch[\"labels\"].to(device)\\n        size = len(batch_images)\\n        \\n        outputs = model(pixel_values=batch_images, labels=batch_labels)\\n        \\n        loss = outputs[0]\\n        epoch_loss.append(loss.item())\\n        loss.backward()\\n        optim.step()\\n        \\n        correct = (torch.argmax(outputs[\"logits\"], dim=1) == batch_labels).sum().item()\\n        acc = (100 * correct) / size\\n        epoch_acc.append(acc)\\n        \\n        # log the training metrics\\n        #if index % record_steps == 0:\\n           # wandb.log({\\'loss\\': loss, \"acc\" : acc})\\n        \\n    \\n    # calculate summary stats for each epoch \\n    avg_accuracy = (sum(epoch_acc) / len(epoch_acc))\\n    avg_loss = (sum(epoch_loss) / len(epoch_loss))\\n    \\n    # we decay the loss over time \\n    scheduler.step()\\n    \\n    # save checkpoints using torchscript \\n    if epoch % save_checkpoint == 0:\\n        model.save_pretrained(f\"Epoch {epoch}\")\\n    \\n    # finding validation accuracy and loss\\n    val_acc, val_loss = evaluate(model, val_loader, val_dataset)\\n    #wandb.log({\\'validation accuracy\\': val_acc, \"validation loss\" : val_loss})\\n    \\n    # summary stats at the end of the episode\\n    print(\"evaluating on validation set\")\\n    print(f\"val loss: {round(val_loss, 4)}, val acc: {round(val_acc, 4)}%\")\\n    print(f\"Epoch: {epoch}  avg loss: {round(avg_loss, 4)} avg acc: {round(avg_accuracy, 4)}%\")'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tqdm = partial(tqdm, position=0, leave=True)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # storing loss and accuracy across the epoch\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    \n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for index, batch in enumerate(tqdm(train_loader)):\n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # extract images and labels from batch\n",
    "        batch_images = batch[\"pixel_values\"].squeeze(1).to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "        size = len(batch_images)\n",
    "        \n",
    "        outputs = model(pixel_values=batch_images, labels=batch_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        correct = (torch.argmax(outputs[\"logits\"], dim=1) == batch_labels).sum().item()\n",
    "        acc = (100 * correct) / size\n",
    "        epoch_acc.append(acc)\n",
    "        \n",
    "        # log the training metrics\n",
    "        #if index % record_steps == 0:\n",
    "           # wandb.log({'loss': loss, \"acc\" : acc})\n",
    "        \n",
    "    \n",
    "    # calculate summary stats for each epoch \n",
    "    avg_accuracy = (sum(epoch_acc) / len(epoch_acc))\n",
    "    avg_loss = (sum(epoch_loss) / len(epoch_loss))\n",
    "    \n",
    "    # we decay the loss over time \n",
    "    scheduler.step()\n",
    "    \n",
    "    # save checkpoints using torchscript \n",
    "    if epoch % save_checkpoint == 0:\n",
    "        model.save_pretrained(f\"Epoch {epoch}\")\n",
    "    \n",
    "    # finding validation accuracy and loss\n",
    "    val_acc, val_loss = evaluate(model, val_loader, val_dataset)\n",
    "    #wandb.log({'validation accuracy': val_acc, \"validation loss\" : val_loss})\n",
    "    \n",
    "    # summary stats at the end of the episode\n",
    "    print(\"evaluating on validation set\")\n",
    "    print(f\"val loss: {round(val_loss, 4)}, val acc: {round(val_acc, 4)}%\")\n",
    "    print(f\"Epoch: {epoch}  avg loss: {round(avg_loss, 4)} avg acc: {round(avg_accuracy, 4)}%\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_acc, test_loss = evaluate(model, test_loader, test_dataset)\\nprint(f\"testing acc: {round(test_acc, 4)}%, testing loss: {round(test_loss, 4)}%\")'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''test_acc, test_loss = evaluate(model, test_loader, test_dataset)\n",
    "print(f\"testing acc: {round(test_acc, 4)}%, testing loss: {round(test_loss, 4)}%\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
